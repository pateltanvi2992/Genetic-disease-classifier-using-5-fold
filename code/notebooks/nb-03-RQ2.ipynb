{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49a6303c-8f7f-4cf0-9b3c-6cd2681454ca",
   "metadata": {},
   "source": [
    "# Do region-based explanations align with HPO terms labeled by clinicians?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c2d922f-2ca4-4ec8-a8cd-976b361b3335",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pathlib\n",
    "import random\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pingouin as pg\n",
    "import cv2\n",
    "import torch\n",
    "import torchmetrics\n",
    "\n",
    "sys.path.insert(0,'../')\n",
    "import datasets\n",
    "\n",
    "from zennit.attribution import Gradient, SmoothGrad\n",
    "from zennit.core import Stabilizer\n",
    "from zennit.composites import EpsilonGammaBox, EpsilonPlusFlat\n",
    "from zennit.composites import SpecialFirstLayerMapComposite, NameMapComposite\n",
    "from zennit.image import imgify, imsave\n",
    "from zennit.rules import Epsilon, ZPlus, ZBox, Norm, Pass, Flat\n",
    "from zennit.types import Convolution, Activation, AvgPool, Linear as AnyLinear\n",
    "from zennit.types import BatchNorm, MaxPool\n",
    "from zennit.torchvision import VGGCanonizer, ResNetCanonizer\n",
    "from zennit.attribution import Gradient, SmoothGrad, IntegratedGradients\n",
    "from zennit.composites import GuidedBackprop, ExcitationBackprop, DeconvNet, EpsilonPlus, EpsilonPlusFlat, EpsilonAlpha2Beta1, EpsilonAlpha2Beta1Flat, EpsilonGammaBox\n",
    "from zennit.image import imgify, imsave\n",
    "from captum.attr import Occlusion\n",
    "from captum.attr import DeepLift, GuidedGradCam\n",
    "from captum.attr import LayerDeepLift, LayerGradCam, LayerAttribution, LayerGradientShap\n",
    "from pytorch_grad_cam.utils.image import show_cam_on_image # TODO: later remove dependency\n",
    "from skimage.measure import label\n",
    "\n",
    "from loguru import logger\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e52c230-af3c-4301-a7c1-6e08ca1257c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_regions(segmap, filename):\n",
    "    # (1)\n",
    "    # There is some noisy predictions in segmentation: use only the largest connected component's region for each segment.\n",
    "    # Try to get rid of those unreliable parts and create region bounding boxes around the most reliable segment.\n",
    "    def get_largest_connected_components(segmentation):\n",
    "        labels = label(segmentation)\n",
    "        assert( np.unique(label(segmap)).size>1)\n",
    "        largestCC = labels == np.argmax(np.bincount(labels.flat)[1:])+1\n",
    "        return largestCC\n",
    "\n",
    "    group_indices = [[2,4,3,5], [6], [7,9]] # 8,\n",
    "    group_names = ['eye', 'nose', 'mouth']\n",
    "    bboxes =  []\n",
    "    for indices in group_indices:\n",
    "        bb = []\n",
    "        for i in indices:\n",
    "            # Get bounding boxes of all segments, i.e., left-eye, left-eyebrow, nose, and so on...\n",
    "            if np.sum(segmap==i)!=0:\n",
    "                img_vis = get_largest_connected_components(label(segmap==i)).astype(np.uint8)\n",
    "                x1, y1 = list(np.min(np.argwhere(img_vis==1),axis=0)) \n",
    "                x2, y2 = list(np.max(np.argwhere(img_vis==1),axis=0))\n",
    "                bb.append([x1, y1, x2, y2])\n",
    "            else:\n",
    "                print('Check segmentation maps!!! ', filename)\n",
    "        bboxes.append(bb)\n",
    "\n",
    "    # (2) Combine parts of region: \n",
    "    # eye region\n",
    "    # nose region\n",
    "    # mouth region\n",
    "    eye_region =[[np.array(bboxes[0])[0:2,0].min(), np.array(bboxes[0])[0:2,1].min(), np.array(bboxes[0])[0:2,2].max(), np.array(bboxes[0])[0:2,3].max()],\n",
    "                 [np.array(bboxes[0])[2:,0].min(), np.array(bboxes[0])[2:,1].min(), np.array(bboxes[0])[2:,2].max(), np.array(bboxes[0])[2:,3].max()],\n",
    "                 [np.array(bboxes[0])[[0,2],0].min(), np.array(bboxes[0])[[0,2],1].min(), np.array(bboxes[0])[[0,2],2].max(), np.array(bboxes[0])[[0,2],3].max()]]\n",
    "    mouth_region = [[np.array(bboxes[2])[:,0].min(), np.array(bboxes[2])[:,1].min(), np.array(bboxes[2])[:,2].max(), np.array(bboxes[2])[:,3].max()]]\n",
    "    nose_region = bboxes[1]\n",
    "    maps = np.zeros(shape=(224,224,3), dtype=np.uint8)\n",
    "    for index, region_bb in enumerate([eye_region, nose_region, mouth_region]):\n",
    "        for bb in region_bb:\n",
    "            x1,y1,x2,y2 = bb\n",
    "            maps[x1:x2, y1:y2, index]=1 \n",
    "    return maps\n",
    "\n",
    "# https://github.com/jacobgil/pytorch-grad-cam/blob/2183a9cbc1bd5fc1d8e134b4f3318c3b6db5671f/pytorch_grad_cam/utils/image.py#L33\n",
    "def show_cam_on_image(img: np.ndarray,\n",
    "                      mask: np.ndarray,\n",
    "                      use_rgb: bool = False,\n",
    "                      colormap: int = cv2.COLORMAP_JET,\n",
    "                      image_weight: float = 0.5) -> np.ndarray:\n",
    "    heatmap = cv2.applyColorMap(np.uint8(255 * mask), colormap)\n",
    "    if use_rgb:\n",
    "        heatmap = cv2.cvtColor(heatmap, cv2.COLOR_BGR2RGB)\n",
    "    heatmap = np.float32(heatmap) / 255\n",
    "\n",
    "    if np.max(img) > 1:\n",
    "        raise Exception(\n",
    "            \"The input image should np.float32 in the range [0, 1]\")\n",
    "\n",
    "    if image_weight < 0 or image_weight > 1:\n",
    "        raise Exception(\n",
    "            f\"image_weight should be in the range [0, 1].\\\n",
    "                Got: {image_weight}\")\n",
    "\n",
    "    cam = (1 - image_weight) * heatmap + image_weight * img\n",
    "    cam = cam / np.max(cam)\n",
    "    return np.uint8(255 * cam)\n",
    "    \n",
    "# https://zennit.readthedocs.io/en/latest/reference/zennit.torchvision.html#zennit.torchvision.ResNetBasicBlockCanonizer\n",
    "#from zennit.torchvision import VGGCanonizer, ResNetCanonizer\n",
    "canonizer = ResNetCanonizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e1b6ecd-1354-48f6-8b07-bf19927ad718",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_relevance(attribution):\n",
    "    # absolute sum over the channels and min-max [0, 1]\n",
    "    relevance = attribution.abs().sum(1).detach().cpu()\n",
    "    relevance = ((relevance - relevance.min()) / (relevance.max() - relevance.min())).cpu().numpy().squeeze()\n",
    "    return relevance\n",
    "\n",
    "def get_relevance(model, input, pred_label_idx, method, num_classes=12):\n",
    "\n",
    "    if method=='Gradient':\n",
    "        # Gradient\n",
    "        attributor = Gradient(model)\n",
    "        target = torch.eye(num_classes).to(device)[[pred_label_idx]] \n",
    "        output, attribution = attributor(input, target)\n",
    "    elif method=='SmoothGrad':\n",
    "        # SmoothGrad\n",
    "        attributor = SmoothGrad(model, noise_level=0.1, n_iter=20)\n",
    "        target = torch.eye(num_classes).to(device)[[pred_label_idx]] \n",
    "        output, attribution = attributor(input, target)\n",
    "    elif method=='IntegratedGradients':\n",
    "        # IntegratedGradients\n",
    "        attributor = IntegratedGradients(model, n_iter=20)\n",
    "        target = torch.eye(num_classes).to(device)[[pred_label_idx]] \n",
    "        output, attribution = attributor(input, target)\n",
    "    elif method=='GuidedBackprop':\n",
    "        # GuidedBackprop\n",
    "        attributor = Gradient(model=model, composite=GuidedBackprop())  \n",
    "        target = torch.eye(num_classes).to(device)[[pred_label_idx]] \n",
    "        output, attribution = attributor(input, target)\n",
    "    elif method=='ExcitationBackprop':\n",
    "        # ExcitationBackprop\n",
    "        attributor = Gradient(model=model, composite=ExcitationBackprop())  \n",
    "        target = torch.eye(num_classes).to(device)[[pred_label_idx]] \n",
    "        output, attribution = attributor(input, target)\n",
    "    elif method=='DeconvNet':\n",
    "        # DeconvNet\n",
    "        attributor = Gradient(model=model, composite=DeconvNet())  \n",
    "        target = torch.eye(num_classes).to(device)[[pred_label_idx]] \n",
    "        output, attribution = attributor(input, target)\n",
    "    elif method=='LRP-EpsilonPlus':\n",
    "        # LRP-EpsilonPlus\n",
    "        attributor = Gradient(model=model, composite=EpsilonPlus(canonizers=[canonizer]))  \n",
    "        target = torch.eye(num_classes).to(device)[[pred_label_idx]] \n",
    "        output, attribution = attributor(input, target)\n",
    "    elif method=='LRP-EpsilonPlusFlat':\n",
    "        # LRP-EpsilonPlusFlat\n",
    "        attributor = Gradient(model=model, composite=EpsilonPlusFlat(canonizers=[canonizer]))  \n",
    "        target = torch.eye(num_classes).to(device)[[pred_label_idx]] \n",
    "        output, attribution = attributor(input, target)\n",
    "    elif method=='LRP-EpsilonAlpha2Beta1':\n",
    "        # LRP-EpsilonAlpha2Beta1\n",
    "        attributor = Gradient(model=model, composite=EpsilonAlpha2Beta1(canonizers=[canonizer]))  \n",
    "        target = torch.eye(num_classes).to(device)[[pred_label_idx]] \n",
    "        output, attribution = attributor(input, target)\n",
    "    elif method=='LRP-EpsilonAlpha2Beta1Flat':\n",
    "        # LRP-EpsilonAlpha2Beta1Flat\n",
    "        attributor = Gradient(model=model, composite=EpsilonAlpha2Beta1Flat(canonizers=[canonizer]))  \n",
    "        target = torch.eye(num_classes).to(device)[[pred_label_idx]] \n",
    "        output, attribution = attributor(input, target)\n",
    "    elif method=='DeepLIFT':\n",
    "        # DeepLIFT\n",
    "        attributor = DeepLift(model)\n",
    "        attribution = attributor.attribute(input, target=int(pred_label_idx))\n",
    "    elif method=='GuidedGradCam':\n",
    "        # GuidedGradCam\n",
    "        attributor = GuidedGradCam(model, model.layer4)\n",
    "        attribution = attributor.attribute(input, target=int(pred_label_idx))\n",
    "    elif method=='LayerDeepLIFT':\n",
    "        # GuidedGradCam\n",
    "        attributor = LayerDeepLift(model, model.layer4)\n",
    "        attribution = attributor.attribute(input, target=int(pred_label_idx))\n",
    "        attribution = LayerAttribution.interpolate(attribution, (224, 224), interpolate_mode='bicubic')\n",
    "    elif method=='LayerGradCam':\n",
    "        # LayerGradCam\n",
    "        attributor = LayerGradCam(model, model.layer4)\n",
    "        attribution = attributor.attribute(input, target=int(pred_label_idx))\n",
    "        attribution = LayerAttribution.interpolate(attribution, (224, 224), interpolate_mode='bicubic')\n",
    "    elif method=='Occlusion':\n",
    "        # Occlusion\n",
    "        attributor = Occlusion(model)\n",
    "        attribution = attributor.attribute(input,\n",
    "                                           strides = (3, 8, 8),\n",
    "                                           target=int(pred_label_idx),\n",
    "                                           sliding_window_shapes=(3,15, 15),\n",
    "                                           baselines=0)        \n",
    "    else:\n",
    "        raise ValueError('XAI saliency map not implemented!')\n",
    "        \n",
    "    return normalize_relevance(attribution)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aad381c5-2a7c-4b40-99bb-767548e0d7d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I do not have these images in the database:\n",
      "['22q11DSSlide150.png', 'KSSlide133.png', 'NSSlide6.png', 'WSSlide316.png']\n"
     ]
    }
   ],
   "source": [
    "region_groups = {}\n",
    "region_groups['overall'] = ['Abnormal facial shape',\n",
    "                            'Long face',\n",
    "                            'Microcephaly', \n",
    "                            'Hypopigmentation of the skin', \n",
    "                            'Elfin facies',\n",
    "                            'Narrow face',\n",
    "                            'Coarse facial features',\n",
    "                            'Triangular face',\n",
    "                            'Midface retrusion',\n",
    "                            'Webbed neck']\n",
    "region_groups['hair'] = ['Fair hair']\n",
    "region_groups['eye'] = ['Epicanthus',\n",
    "                        'Upslanted palpebral fissure',\n",
    "                        'Abnormal eyelid morphology',\n",
    "                        'Ptosis',\n",
    "                        'Telecanthus',\n",
    "                        'Strabismus',\n",
    "                        'Iris hypopigmentation',\n",
    "                        'Blepharophimosis',\n",
    "                        'Downslanted palpebral fissures',\n",
    "                        'Hypertelorism',\n",
    "                        'Proptosis',\n",
    "                        'Highly arched eyebrow',\n",
    "                        'Sparse lateral eyebrow',\n",
    "                        'Long eyelashes',\n",
    "                        'Eversion of lateral third of lower eyelids']\n",
    "region_groups['nose'] = ['Prominent nasal bridge',\n",
    "                         'Wide nasal bridge',\n",
    "                         'Bulbous nose',\n",
    "                         'Short nose',\n",
    "                         'Short columella']\n",
    "region_groups['ears'] = ['Low-set ears',\n",
    "                         'Overfolded helix',\n",
    "                         'Small earlobe',\n",
    "                         'Low-set posteriorly rotated ears',\n",
    "                         'Protruding ear',\n",
    "                         'Thickened helices',\n",
    "                         'Macrotia']\n",
    "region_groups['mouth'] = ['Long philtrum',\n",
    "                          'Wide mouth',\n",
    "                          'Protruding tongue',\n",
    "                          'Thick lower lip vermilion',\n",
    "                          'Everted lower lip vermilion',\n",
    "                          'Open bite',\n",
    "                          'Widely spaced teeth',\n",
    "                          'Microdontia',\n",
    "                          'Abnormality of the dentition']\n",
    "region_groups['chin'] = ['Pointed chin']\n",
    "region_groups['forehead'] = ['High forehead',\n",
    "                             'Broad forehead']\n",
    "hpo_terms = []\n",
    "for group in region_groups:\n",
    "    hpo_terms += region_groups[group]\n",
    "hpo_terms = list(set(hpo_terms))\n",
    "\n",
    "dataset_folder = '/media/omersumer/DATA/databases/NIH-Faces'\n",
    "hpo_terms_from_csv_files = []\n",
    "image_names = []\n",
    "for syndrome in ['22q11DS', 'Angelman', 'KS', 'NS', 'WS']:\n",
    "    df = pd.read_csv(pathlib.Path(dataset_folder, 'metadata', 'hpo-annotations', '%s.csv'%syndrome))\n",
    "    hpo_terms_from_csv_files += df.keys()[2:-1].to_list()\n",
    "    image_names += df['image_name'].to_list()\n",
    "assert(len([i for i in set(hpo_terms_from_csv_files) if i not in hpo_terms])==0)\n",
    "\n",
    "df = pd.read_csv(pathlib.Path(dataset_folder, 'metadata', 'partitions.csv'))\n",
    "\n",
    "print('I do not have these images in the database:')\n",
    "missing_images = [i for i in image_names if i not in df['image_name'].to_list()]\n",
    "print(missing_images)\n",
    "\n",
    "hpo_occurrence = np.zeros(shape=(len(image_names), len(hpo_terms)), dtype=np.float32)\n",
    "for i in range(0, len(image_names)):\n",
    "    syndrome = image_names[i].split('Slide')[0]\n",
    "    df = pd.read_csv(pathlib.Path(dataset_folder, 'metadata', 'hpo-annotations', '%s.csv'%syndrome))\n",
    "    index = np.argwhere(df['image_name'].to_numpy()==image_names[i]).squeeze()\n",
    "    #print(image_names[i], syndrome, index)\n",
    "    \n",
    "    for key in list(df.iloc[index][2:-1].keys()):\n",
    "        #print(key)\n",
    "        j = np.argwhere(np.array(hpo_terms) == key).squeeze()\n",
    "        assert(j.size==1)\n",
    "\n",
    "        # Annotations: 0, X, 2X, 3X\n",
    "        if df.iloc[index][key] is np.nan:\n",
    "            val = 0 \n",
    "        elif df.iloc[index][key]=='1X':\n",
    "            val = 0.333\n",
    "        elif df.iloc[index][key]=='2X':\n",
    "            val = 0.666\n",
    "        elif df.iloc[index][key]=='3X':\n",
    "            val = 1.000\n",
    "        else:\n",
    "            raise Exception('Check values: %s, %s, %s'%(syndrome, image_names[i], key))\n",
    "\n",
    "        hpo_occurrence[i, j] = val\n",
    "        \n",
    "df = pd.DataFrame({'image_name':image_names, 'category':[i.split('Slide')[0]  for i in image_names]})\n",
    "df = pd.concat([df, pd.DataFrame(hpo_occurrence)], axis=1)\n",
    "df = df.drop([i for i in range(0, df.shape[0]) if df.iloc[i]['image_name'] in missing_images]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d10124ca-e10d-4435-8b9e-300b14398735",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "device = 1\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "torch.backends.cudnn.enabled = False\n",
    "torch.use_deterministic_algorithms = True\n",
    "device = torch.device(\"cuda:%d\"%device if torch.cuda.is_available() and device>=0 else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3fdb905a-55c9-4a10-a30c-f3e90f53f543",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing (Gradient)\n",
      "Processing (SmoothGrad)\n",
      "Processing (IntegratedGradients)\n",
      "Processing (GuidedBackprop)\n",
      "Processing (ExcitationBackprop)\n",
      "Processing (DeconvNet)\n",
      "Processing (LRP-EpsilonPlus)\n",
      "Processing (LRP-EpsilonPlusFlat)\n",
      "Processing (LRP-EpsilonAlpha2Beta1)\n",
      "Processing (LRP-EpsilonAlpha2Beta1Flat)\n",
      "Processing (DeepLIFT)\n",
      "Processing (GuidedGradCam)\n",
      "Processing (LayerDeepLIFT)\n",
      "Processing (LayerGradCam)\n",
      "Processing (Occlusion)\n"
     ]
    }
   ],
   "source": [
    "def evaluate_xai_maps(fold, xai_method):\n",
    "    \n",
    "    dataset_folder = '/media/omersumer/DATA/databases/NIH-Faces'\n",
    "    project_root = './'\n",
    "    mean_bgr = {'fold-1':[112, 123, 147],\n",
    "                'fold-2':[112, 123, 147], \n",
    "                'fold-3':[112, 122, 147], \n",
    "                'fold-4':[112, 123, 147], \n",
    "                'fold-5':[112, 123, 147]}\n",
    "    num_classes = 12\n",
    "    categories = ['22q11DS', 'Angelman', 'BWS', 'CdLS', 'Down', 'KS', 'NS', 'PWS', 'RSTS1', 'Unaffected', 'WHS', 'WS']\n",
    "\n",
    "    # Test dataset\n",
    "    image_size = 224 # we use only VGG-Face2 pretrained ResNet50\n",
    "    test_dataset = datasets.NIHFacesDataset(root_dir=dataset_folder,\n",
    "                                            metadata_file='../metadata/partitions.csv',\n",
    "                                            fold=fold, \n",
    "                                            split='test', \n",
    "                                            mean_bgr=mean_bgr[fold],\n",
    "                                            image_size=image_size,\n",
    "                                            flip=False)\n",
    "    num_samples = len(test_dataset)\n",
    "\n",
    "    # Load the models\n",
    "    val_acc = np.array([float(s.as_posix().split('-test_accuracy-')[-1].replace('.pt','')) for s in list(pathlib.Path('../results/%s/%s'%('VGGFace2_ResNet50', fold)).glob('epoch-*.pt'))])\n",
    "    model_path = list(pathlib.Path('../results/%s/%s'%('VGGFace2_ResNet50', fold)).glob('epoch-*.pt'))[np.argmax(val_acc)].as_posix()\n",
    "    #logger.info('model_path: %s'%model_path) \n",
    "    model = torch.load(model_path)\n",
    "    model.to(device)\n",
    "    eval_mode = model.eval()\n",
    "    \n",
    "    results = pd.DataFrame(columns=['backbone', 'fold', 'image_name', 'method', 'eye_xai', 'nose_xai', 'mouth_xai', 'eye_hpo', 'nose_hpo', 'mouth_hpo'] + ['label','predicted']+['p_%s'%i for i in categories])\n",
    "    n = 0\n",
    "    \n",
    "    for index in range(0, num_samples):\n",
    "\n",
    "        # load images and labels\n",
    "        image, gt_label, segmap, filename, landmark = test_dataset.__getitem__(index)\n",
    "        input = image.unsqueeze(0).to(device)\n",
    "        input.requires_grad = True\n",
    "\n",
    "        output = model(input)\n",
    "        prob_output = torch.nn.functional.softmax(output, dim=1)\n",
    "        prob_output = prob_output.detach().cpu().numpy().squeeze()\n",
    "\n",
    "        pred_label_idx = np.argmax(prob_output)\n",
    "        predicted_label = categories[pred_label_idx]\n",
    "\n",
    "        # get 3-channel RGB map for eye, nose and mouth regions\n",
    "        def get_largest_connected_components(segmentation):\n",
    "            labels = label(segmentation)\n",
    "            assert( np.unique(label(segmap)).size>1)\n",
    "            largestCC = labels == np.argmax(np.bincount(labels.flat)[1:])+1\n",
    "            return largestCC\n",
    "\n",
    "        segmap = cv2.resize(segmap, (224, 224))\n",
    "        right_eye = [list(np.argwhere(get_largest_connected_components(segmap==i)==True).min(axis=0)) + list(np.argwhere(get_largest_connected_components(segmap==i)==True).max(axis=0)) for i in [2,4]]\n",
    "        left_eye  = [list(np.argwhere(get_largest_connected_components(segmap==i)==True).min(axis=0)) + list(np.argwhere(get_largest_connected_components(segmap==i)==True).max(axis=0)) for i in [3,5]]\n",
    "        right_eye, left_eye = np.array(right_eye), np.array(left_eye)\n",
    "\n",
    "        reye_x1, reye_y1, reye_x2, reye_y2 = right_eye[:,1].min(), right_eye[:,0].min(), right_eye[:,3].max(), right_eye[:,2].max()\n",
    "        leye_x1, leye_y1, leye_x2, leye_y2 = left_eye[:,1].min(),  left_eye[:,0].min(),  left_eye[:,3].max(),  left_eye[:,2].max()\n",
    "\n",
    "        reye_w,  reye_h = reye_x2-reye_x1, reye_y2-reye_y1\n",
    "        reye_x1, reye_y1, reye_x2, reye_y2 = reye_x1-int(0.2*reye_w), reye_y1-int(0.2*reye_h), reye_x2+int(0.1*reye_w), reye_y2+int(0.2*reye_h)\n",
    "        leye_w,  leye_h = leye_x2-leye_x1, leye_y2-leye_y1\n",
    "        leye_x1, leye_y1, leye_x2, leye_y2 = leye_x1-int(0.1*leye_w), leye_y1-int(0.2*leye_h), leye_x2+int(0.2*leye_w), leye_y2+int(0.2*leye_h) \n",
    "\n",
    "        nose_x1, nose_y1, nose_x2, nose_y2  = list(np.argwhere(get_largest_connected_components(segmap==6)==True).min(axis=0))[::-1] + list(np.argwhere(get_largest_connected_components(segmap==6)==True).max(axis=0))[::-1]  \n",
    "        nose_y1 = min(reye_y1, leye_y1) + int(0.3 * min(reye_h, leye_h))\n",
    "\n",
    "        mouth = np.array([list(np.argwhere(get_largest_connected_components(segmap==i)==True).min(axis=0)) + list(np.argwhere(get_largest_connected_components(segmap==i)==True).max(axis=0)) for i in [7,9]])\n",
    "        mouth_x1, mouth_y1, mouth_x2, mouth_y2  = mouth[:,1].min(), mouth[:,0].min(), mouth[:,3].max(), mouth[:,2].max()\n",
    "        mouth_y2 += int(0.5 * (mouth_y1 - nose_y2))\n",
    "        mouth_y1, nose_y2 = 2*[int(0.5 * (mouth_y1 + nose_y2))]\n",
    "        mouth_x1, mouth_x2 = mouth_x1-int(0.2*(mouth_x2-mouth_x1)), mouth_x2+int(0.2*(mouth_x2-mouth_x1))\n",
    "        mouth_y2 = min(mouth_y2, 224)\n",
    "\n",
    "        region_map = np.zeros(shape=(224,224,3), dtype=np.uint8)\n",
    "        region_map[reye_y1:reye_y2, reye_x1:reye_x2, 0] = 1\n",
    "        region_map[leye_y1:leye_y2, leye_x1:leye_x2, 0] = 1\n",
    "        region_map[nose_y1:nose_y2, nose_x1:nose_x2, 1] = 1\n",
    "        region_map[mouth_y1:mouth_y2, mouth_x1:mouth_x2, 2] = 1\n",
    "        for i in range(0, 224):\n",
    "            for j in range(0, 224):\n",
    "                region_map[i,j,:] = [1,0,0]  if (region_map[i,j,:]==[1,1,0]).all() else region_map[i,j,:]\n",
    "\n",
    "\n",
    "        # get saliency map\n",
    "        relevance_map = get_relevance(model, input, pred_label_idx, xai_method)\n",
    "        relevance_map = cv2.resize(relevance_map, (224, 224))\n",
    "        relevance_map = region_map[:,:,0] * relevance_map + \\\n",
    "                        region_map[:,:,1] * relevance_map + \\\n",
    "                        region_map[:,:,2] * relevance_map\n",
    "\n",
    "        # saliency region coefficients\n",
    "        eye_xai   = np.sum(relevance_map * region_map[:,:,0]) / np.sum(region_map[:,:,0]!=0)\n",
    "        nose_xai  = np.sum(relevance_map * region_map[:,:,1]) / np.sum(region_map[:,:,1]!=0)\n",
    "        mouth_xai = np.sum(relevance_map * region_map[:,:,2]) / np.sum(region_map[:,:,2]!=0)\n",
    "\n",
    "        # read HPO Annotations and calculate HPO-base coefficients\n",
    "        ii = [idx for idx, val in enumerate(df['image_name']) if val.replace('.png','')==filename]\n",
    "        ii = ii[0] if len(ii)==1 else None\n",
    "        t = ['%2.3f'%(i) for i in df.iloc[ii][2:].to_list()]\n",
    "        t_per_image =[]\n",
    "        for i in range(0, 50):\n",
    "            if t[i]!='0.000':\n",
    "                t_per_image.append('%s'%(hpo_terms[i]))\n",
    "        labeled_terms = [(key, t_per_image[ii]) for key in region_groups.keys() for ii in range(0, len(t_per_image)) if t_per_image[ii] in region_groups[key] ]\n",
    "\n",
    "        eye_hpo = np.sum(np.array(df.iloc[ii][2:], dtype=np.float32)[ [idx for idx, val in enumerate(hpo_terms) if val in region_groups['eye']] ])\n",
    "        nose_hpo = np.sum(np.array(df.iloc[ii][2:], dtype=np.float32)[ [idx for idx, val in enumerate(hpo_terms) if val in region_groups['nose']] ])\n",
    "        mouth_hpo = np.sum(np.array(df.iloc[ii][2:], dtype=np.float32)[ [idx for idx, val in enumerate(hpo_terms) if val in region_groups['mouth']] ])\n",
    "\n",
    "        results.loc[n] = ['VGGFace2_ResNet50'] + [fold] + [filename] + [xai_method] + [eye_xai, nose_xai, mouth_xai, eye_hpo, nose_hpo, mouth_hpo] + [test_dataset.categories[pred_label_idx]] + [test_dataset.categories[int(gt_label)]] + ['%2.3f'%i for i in list(prob_output)]\n",
    "        n += 1\n",
    "\n",
    "        \n",
    "    results_stats = pd.DataFrame(columns=['method', 'fold', 'region', 'r', 'CI95%', 'p-val', 'power'] )\n",
    "    results_stats.loc[0] = [xai_method] + [fold] + ['eye'] + [round(float(pg.corr( results['eye_xai'],results['eye_hpo'], method='spearman')['r']), 5)] +\\\n",
    "                                           [str(list(pg.corr( results['eye_xai'],results['eye_hpo'], method='spearman')['CI95%'][0])).replace(',',';')] +\\\n",
    "                                           [float(pg.corr( results['eye_xai'],results['eye_hpo'], method='spearman')['p-val'])] + \\\n",
    "                                           [round(float(pg.corr( results['eye_xai'],results['eye_hpo'], method='spearman')['power']), 5)]\n",
    "\n",
    "    results_stats.loc[1] = [xai_method] + [fold] + ['nose'] + [round(float(pg.corr( results['nose_xai'],results['nose_hpo'], method='spearman')['r']), 5)] +\\\n",
    "                                             [str(list(pg.corr( results['nose_xai'],results['nose_hpo'], method='spearman')['CI95%'][0])).replace(',',';')] +\\\n",
    "                                             [float(pg.corr( results['nose_xai'],results['nose_hpo'], method='spearman')['p-val'])] + \\\n",
    "                                             [round(float(pg.corr( results['nose_xai'],results['nose_hpo'], method='spearman')['power']), 5)]\n",
    "\n",
    "    results_stats.loc[2] = [xai_method] + [fold] + ['mouth'] + [round(float(pg.corr( results['mouth_xai'],results['mouth_hpo'], method='spearman')['r']), 5)] +\\\n",
    "                                             [str(list(pg.corr( results['mouth_xai'],results['mouth_hpo'], method='spearman')['CI95%'][0])).replace(',',';')] +\\\n",
    "                                             [float(pg.corr( results['mouth_xai'],results['mouth_hpo'], method='spearman')['p-val'])] + \\\n",
    "                                             [round(float(pg.corr( results['mouth_xai'],results['mouth_hpo'], method='spearman')['power']), 5)]\n",
    "    #display(results_stats)\n",
    "\n",
    "    return results, results_stats\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "xai_methods = ['Gradient', 'SmoothGrad', 'IntegratedGradients', 'GuidedBackprop', 'ExcitationBackprop', 'DeconvNet',\n",
    "               'LRP-EpsilonPlus', 'LRP-EpsilonPlusFlat', 'LRP-EpsilonAlpha2Beta1', 'LRP-EpsilonAlpha2Beta1Flat',\n",
    "               'DeepLIFT', 'GuidedGradCam', 'LayerDeepLIFT', 'LayerGradCam', 'Occlusion']\n",
    "\n",
    "\n",
    "rq2_correlation_analysis = pd.DataFrame(columns=['method', 'eye', 'nose', 'mouth'] )\n",
    "\n",
    "\n",
    "for xai_method in xai_methods:\n",
    "    print('Processing (%s)'%xai_method)\n",
    "    \n",
    "    corr_res = pd.DataFrame(columns=['method', 'fold', 'region', 'r', 'CI95%', 'p-val', 'power'] ) \n",
    "    for fold in ['fold-1', 'fold-2', 'fold-3', 'fold-4', 'fold-5']:\n",
    "        _, stats = evaluate_xai_maps(fold, xai_method)\n",
    "        corr_res = pd.concat([corr_res, stats])\n",
    "\n",
    "    result = []\n",
    "    result.append(xai_method)\n",
    "    for region in ['eye', 'nose', 'mouth']:\n",
    "        result.append('%2.3f +/- %2.3f'%(corr_res[corr_res['region']==region]['r'].mean(), \n",
    "                                         corr_res[corr_res['region']==region]['r'].std()))\n",
    "    #display(pd.DataFrame([result], columns=['method', 'eye', 'nose', 'mouth' ])) \n",
    "    \n",
    "    rq2_correlation_analysis = pd.concat([rq2_correlation_analysis, pd.DataFrame([result], columns=['method', 'eye', 'nose', 'mouth' ])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "87bf9b2c-021e-4aeb-a7ff-a9aa18da0e23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>method</th>\n",
       "      <th>eye</th>\n",
       "      <th>nose</th>\n",
       "      <th>mouth</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Gradient</td>\n",
       "      <td>0.146 +/- 0.042</td>\n",
       "      <td>0.078 +/- 0.061</td>\n",
       "      <td>0.335 +/- 0.192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SmoothGrad</td>\n",
       "      <td>0.044 +/- 0.149</td>\n",
       "      <td>0.034 +/- 0.070</td>\n",
       "      <td>0.485 +/- 0.136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>IntegratedGradients</td>\n",
       "      <td>-0.039 +/- 0.145</td>\n",
       "      <td>0.023 +/- 0.040</td>\n",
       "      <td>0.425 +/- 0.161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GuidedBackprop</td>\n",
       "      <td>0.054 +/- 0.091</td>\n",
       "      <td>-0.046 +/- 0.065</td>\n",
       "      <td>0.331 +/- 0.112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ExcitationBackprop</td>\n",
       "      <td>0.021 +/- 0.202</td>\n",
       "      <td>-0.006 +/- 0.072</td>\n",
       "      <td>0.345 +/- 0.078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DeconvNet</td>\n",
       "      <td>-0.102 +/- 0.066</td>\n",
       "      <td>-0.033 +/- 0.090</td>\n",
       "      <td>-0.414 +/- 0.097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LRP-EpsilonPlus</td>\n",
       "      <td>-0.009 +/- 0.055</td>\n",
       "      <td>-0.015 +/- 0.061</td>\n",
       "      <td>0.403 +/- 0.075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LRP-EpsilonPlusFlat</td>\n",
       "      <td>0.048 +/- 0.079</td>\n",
       "      <td>-0.011 +/- 0.088</td>\n",
       "      <td>0.422 +/- 0.090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LRP-EpsilonAlpha2Beta1</td>\n",
       "      <td>0.059 +/- 0.152</td>\n",
       "      <td>0.017 +/- 0.148</td>\n",
       "      <td>0.380 +/- 0.091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LRP-EpsilonAlpha2Beta1Flat</td>\n",
       "      <td>-0.016 +/- 0.124</td>\n",
       "      <td>-0.008 +/- 0.153</td>\n",
       "      <td>0.429 +/- 0.086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DeepLIFT</td>\n",
       "      <td>0.163 +/- 0.119</td>\n",
       "      <td>0.049 +/- 0.063</td>\n",
       "      <td>0.512 +/- 0.097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GuidedGradCam</td>\n",
       "      <td>0.331 +/- 0.129</td>\n",
       "      <td>0.074 +/- 0.121</td>\n",
       "      <td>0.449 +/- 0.123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LayerDeepLIFT</td>\n",
       "      <td>0.087 +/- 0.121</td>\n",
       "      <td>0.016 +/- 0.136</td>\n",
       "      <td>0.410 +/- 0.213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LayerGradCam</td>\n",
       "      <td>0.441 +/- 0.085</td>\n",
       "      <td>0.259 +/- 0.215</td>\n",
       "      <td>0.529 +/- 0.137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Occlusion</td>\n",
       "      <td>0.111 +/- 0.125</td>\n",
       "      <td>-0.145 +/- 0.117</td>\n",
       "      <td>0.512 +/- 0.090</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       method               eye              nose  \\\n",
       "0                    Gradient   0.146 +/- 0.042   0.078 +/- 0.061   \n",
       "0                  SmoothGrad   0.044 +/- 0.149   0.034 +/- 0.070   \n",
       "0         IntegratedGradients  -0.039 +/- 0.145   0.023 +/- 0.040   \n",
       "0              GuidedBackprop   0.054 +/- 0.091  -0.046 +/- 0.065   \n",
       "0          ExcitationBackprop   0.021 +/- 0.202  -0.006 +/- 0.072   \n",
       "0                   DeconvNet  -0.102 +/- 0.066  -0.033 +/- 0.090   \n",
       "0             LRP-EpsilonPlus  -0.009 +/- 0.055  -0.015 +/- 0.061   \n",
       "0         LRP-EpsilonPlusFlat   0.048 +/- 0.079  -0.011 +/- 0.088   \n",
       "0      LRP-EpsilonAlpha2Beta1   0.059 +/- 0.152   0.017 +/- 0.148   \n",
       "0  LRP-EpsilonAlpha2Beta1Flat  -0.016 +/- 0.124  -0.008 +/- 0.153   \n",
       "0                    DeepLIFT   0.163 +/- 0.119   0.049 +/- 0.063   \n",
       "0               GuidedGradCam   0.331 +/- 0.129   0.074 +/- 0.121   \n",
       "0               LayerDeepLIFT   0.087 +/- 0.121   0.016 +/- 0.136   \n",
       "0                LayerGradCam   0.441 +/- 0.085   0.259 +/- 0.215   \n",
       "0                   Occlusion   0.111 +/- 0.125  -0.145 +/- 0.117   \n",
       "\n",
       "              mouth  \n",
       "0   0.335 +/- 0.192  \n",
       "0   0.485 +/- 0.136  \n",
       "0   0.425 +/- 0.161  \n",
       "0   0.331 +/- 0.112  \n",
       "0   0.345 +/- 0.078  \n",
       "0  -0.414 +/- 0.097  \n",
       "0   0.403 +/- 0.075  \n",
       "0   0.422 +/- 0.090  \n",
       "0   0.380 +/- 0.091  \n",
       "0   0.429 +/- 0.086  \n",
       "0   0.512 +/- 0.097  \n",
       "0   0.449 +/- 0.123  \n",
       "0   0.410 +/- 0.213  \n",
       "0   0.529 +/- 0.137  \n",
       "0   0.512 +/- 0.090  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(rq2_correlation_analysis)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
